# üß† ETL Preditiva ‚Äî Data Engineering & Analytics Pipeline

[![Hadoop](https://img.shields.io/badge/Hadoop-3.3.6-yellow.svg)]()
[![Airflow](https://img.shields.io/badge/Airflow-2.8.2-blue.svg)]()
[![Python](https://img.shields.io/badge/Python-3.11-green.svg)]()
[![SQL Server](https://img.shields.io/badge/SQL%20Server-2019-red.svg)]()
[![Docker](https://img.shields.io/badge/Docker-Compose-blue.svg)]()

Este projeto implementa um **pipeline completo de engenharia de dados**, incluindo:

- Orquestra√ß√£o com Airflow  
- Data Lake em Hadoop HDFS  
- Data Warehouse em SQL Server  
- Extra√ß√£o de dados de PostgreSQL  
- ETL em Python  
- Gera√ß√£o de **slices anal√≠ticos** para Machine Learning  
- Armazenamento particionado em Parquet  

Tudo containerizado via **Docker Compose**.

---

## üìê Arquitetura Geral

![Arquitetura](./docs/arquitetura_pipeline_etl_preditivo_v2.png)

---

## üöÄ Funcionalidades Principais

### üîπ ETL Completo (PostgreSQL ‚Üí SQL Server DW)
- Extra√ß√£o incremental
- Transforma√ß√µes e limpeza
- Modelagem dimensional
- Tabelas Dimens√£o:
  - Cliente, Vendedor, Produto, Categoria, Fornecedor
  - Forma Pagamento, Situa√ß√£o T√≠tulo
  - Nota Fiscal, Parcela
- Tabelas Fato:
  - FatoVendas
  - FatoPagamentos
  - FatoRecebimentos

### üîπ Gera√ß√£o de Slices Anal√≠ticos
Criados automaticamente em CSV:

- **Recompra em 90 dias** (classifica√ß√£o)
- **Atraso >30 dias** (classifica√ß√£o)
- **Valor em 30 dias** (regress√£o)

### Slices Anal√≠ticos
Gerados automaticamente em CSV e enviados ao HDFS como Parquet particionado.

### üîπ Data Lake em Hadoop HDFS
Cada slice √© convertido em **Parquet particionado**, ex:
Paths:

```
/datalake/analytics/recompra/ano=2025/mes=12/dados.parquet
/datalake/analytics/atrasos/ano=2025/mes=12/dados.parquet
```

### üîπ Orquestra√ß√£o com Airflow
- DAGs individuais para cada dimens√£o/fato
- DAGs de gera√ß√£o e upload dos slices
- **DAG Master** para orquestrar tudo

---

## üõ†Ô∏è Tecnologias

| Camada | Tecnologias |
|--------|-------------|
| Orquestra√ß√£o | Apache Airflow 2.8 |
| Data Lake | Hadoop HDFS 3.3.6 |
| DW | SQL Server 2019 |
| OLTP | PostgreSQL |
| ETL | Python (Pandas, PyArrow, SQLAlchemy, hdfs) |
| Deploy | Docker / Docker Compose |

---

## üìÇ Estrutura do Projeto
etl_preditiva/
‚îÇ
‚îú‚îÄ‚îÄ airflow/                     # DAGs e scripts de ETL
‚îú‚îÄ‚îÄ docs/                        # Arquitetura e Diagramas .png
‚îú‚îÄ‚îÄ hadoop/                      # Configura√ß√£o do HDFS
‚îú‚îÄ‚îÄ sqlserver/                   # Scripts e init .bak
‚îú‚îÄ‚îÄ postgres/                    # Scripts do banco origem
‚îú‚îÄ‚îÄ data_slices_csv/             # Sa√≠da intermedi√°ria
‚îú‚îÄ‚îÄ models/                      # Modelos preditivos (futuro)
‚îú‚îÄ‚îÄ logs/                        # Logs gerais
‚îú‚îÄ‚îÄ src/                         # C√≥digo da aplica√ß√£o ETL
‚îÇ   ‚îú‚îÄ‚îÄ database.py              # Conex√µes SQL Server e Postgres
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ start.sh / stop.sh           # Scripts de controle
‚îî‚îÄ‚îÄ README.md

---
### ü§ñ Machine Learning (Futuro)
```
Os slices do data lake permitem treinar modelos como:

üîπ Recompra em 90 dias

(Classifica√ß√£o)
Prever se o cliente far√° nova compra.

üîπ Atraso no pagamento (>30 dias)

(Classifica√ß√£o)
Identificar comportamento de inadimpl√™ncia.

üîπ Valor esperado em 30 dias

(Regress√£o)
Prever receita por cliente.

---
```
## ‚ñ∂Ô∏è Como Executar

### 1. Criar ambiente virtual (opcional)

```
bash
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

```
### 2. Senhas arquivo .env
```
 - Modificar o nome .env.exemplo para .env
 - No .env: Configurar as credencias do Postgres_DataDT 
```
### 2. Subir todo o ambiente
```
chmod +x start.sh stop.sh
./start.sh

```
### 2. Acessos
```
- Airflow: http://localhost:8080 
- HDFS: http://localhost:9870  
- YARN: http://localhost:8088  
- SQL Server: localhost:1433  

```
### 3. Criando o bando de dados no SQL Server

```
 3.1 Rodar o script ./etl_preditiva/sqlserver/mssql_backups/projeto_etl_preditivo.sql
  - Nele tem todos os comandos DDL para cria√ß√£o do banco, tabelas e views.
  
 3.2 üíæ Backup SQL Server: Gerar .bak do container para o host

Obs.: Rodar esse comando ap√≥s o banco populado, caso queira deixar sempre uma copia atualizada no host.

docker exec etl_sqlserver /opt/mssql-tools/bin/sqlcmd    -S localhost -U sa -P "YourStrong@Passw0rd"    -Q "BACKUP DATABASE [projeto_etl_preditivo] TO DISK='/var/opt/mssql/backup/projeto_etl_preditivo.bak' WITH INIT, STATS=10"
```

Backup salvo em:

```
sqlserver/mssql_backups/
```

## üß™ Testar HDFS

```
docker exec -it airflow_scheduler python3 - <<EOF
from hdfs import InsecureClient
client = InsecureClient("http://namenode:9870", user="root")
client.write("/teste_airflow_ok.txt", data=b"ok", overwrite=True)
print(client.list("/"))
EOF
```

## üë®‚Äçüíª Autor
Projeto desenvolvido para fins de engenharia de dados, automa√ß√£o e machine learning(ainda em desenvolvimento).
